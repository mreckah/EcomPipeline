[2025-08-25T02:18:56.590+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: daily_pipeline.run_pipeline manual__2025-08-25T02:13:54.400284+00:00 [queued]>
[2025-08-25T02:18:56.597+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: daily_pipeline.run_pipeline manual__2025-08-25T02:13:54.400284+00:00 [queued]>
[2025-08-25T02:18:56.597+0000] {taskinstance.py:1361} INFO - Starting attempt 2 of 2
[2025-08-25T02:18:56.606+0000] {taskinstance.py:1382} INFO - Executing <Task(BashOperator): run_pipeline> on 2025-08-25 02:13:54.400284+00:00
[2025-08-25T02:18:56.612+0000] {standard_task_runner.py:57} INFO - Started process 66 to run task
[2025-08-25T02:18:56.616+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'daily_pipeline', 'run_pipeline', 'manual__2025-08-25T02:13:54.400284+00:00', '--job-id', '278', '--raw', '--subdir', 'DAGS_FOLDER/daily_pipeline_dag.py', '--cfg-path', '/tmp/tmpbiep41vt']
[2025-08-25T02:18:56.616+0000] {standard_task_runner.py:85} INFO - Job 278: Subtask run_pipeline
[2025-08-25T02:18:56.656+0000] {task_command.py:416} INFO - Running <TaskInstance: daily_pipeline.run_pipeline manual__2025-08-25T02:13:54.400284+00:00 [running]> on host 88d490cfafa3
[2025-08-25T02:18:56.715+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='ecommerce' AIRFLOW_CTX_DAG_ID='daily_pipeline' AIRFLOW_CTX_TASK_ID='run_pipeline' AIRFLOW_CTX_EXECUTION_DATE='2025-08-25T02:13:54.400284+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-08-25T02:13:54.400284+00:00'
[2025-08-25T02:18:56.716+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-08-25T02:18:56.717+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', '\n        cd /opt/***\n\n        # Debug: Check what files are available\n        echo "=== Debugging file locations ==="\n        echo "Contents of /opt/***/dags:"\n        ls -la /opt/***/dags/\n        echo "Contents of /opt/***/dags/scripts:"\n        ls -la /opt/***/dags/scripts/ || echo "dags/scripts not found"\n        echo "Searching for pipeline.sh:"\n        find /opt/*** -name "pipeline.sh" -type f || echo "pipeline.sh not found anywhere"\n\n        # The docker-compose mounts ./scripts to /opt/***/dags/scripts, overriding the dags/scripts\n        # So we need to copy our script from the dags mount to a temp location\n\n        # Debug: Test file access first\n        echo "=== Testing file access ==="\n        ls -la /opt/***/data/sales.csv || echo "File not found"\n        [ -f "/opt/***/data/sales.csv" ] && echo "File exists and is readable" || echo "File check failed"\n\n        # Create a temporary script with the correct paths\n        cat > /tmp/pipeline.sh << \'EOF\'\n#!/usr/bin/env bash\nset -euo pipefail\n\n### --------- CONFIG (edit if needed) ---------\nDATASET_LOCAL_PATH="/opt/***/data/sales.csv"\n    NIFI_LOCAL_INBOX="/opt/***/data"\n    HDFS_TARGET_PATH="/marketing/sales/sales.csv"\n\n    # NiFi\n    NIFI_URL="https://localhost:8443"\n    NIFI_USER="admin"\n    NIFI_PASS="admin123456789"\n    NIFI_PROCESS_GROUP_ID="PUT_YOUR_PG_ID_HERE"\n\n    # Spark\n    SPARK_MASTER_CONTAINER="spark-master"\n    SPARK_HOME="/opt/bitnami/spark"\n    SPARK_APP_DIR="${SPARK_HOME}/app"\n    SPARK_APP_PATH="${SPARK_APP_DIR}/marketing_job.py"\n    SPARK_MASTER_URL="spark://spark-master:7077"\n\n    # HDFS (inside namenode container)\n    NAMENODE_CONTAINER="namenode"\n\n    # Postgres (warehouse-db) verification\n    PG_CONTAINER="warehouse-db"\n    PG_DB="warehouse"\n    PG_USER="warehouse_user"\n    PG_PASS="warehouse_pass"\n    PG_TABLE="sales_data"\n\n    # Grafana (optional reload if you use provisioning)\n    GRAFANA_URL="http://localhost:3000"\n    GRAFANA_USER="admin"\n    GRAFANA_PASS="admin"\n\n    echo "==> 1) Checking dataset exists: ${DATASET_LOCAL_PATH}"\n    if [[ ! -f "${DATASET_LOCAL_PATH}" ]]; then\n    echo "ERROR: ${DATASET_LOCAL_PATH} not found."\n    exit 1\n    fi\n\n    echo "==> 2) Dataset already in correct location: ${NIFI_LOCAL_INBOX}"\n    echo "File size: $(ls -lh ${DATASET_LOCAL_PATH} | awk \'{print $5}\')"\n\n    echo "==> 3) Starting NiFi process group to ingest CSV into HDFS"\n    curl -k -u "${NIFI_USER}:${NIFI_PASS}" -X PUT     "${NIFI_URL}/nifi-api/flow/process-groups/${NIFI_PROCESS_GROUP_ID}"     -H "Content-Type: application/json"     -d "{"id":"${NIFI_PROCESS_GROUP_ID}","state":"RUNNING"}" >/dev/null || echo "NiFi step skipped"\n\n    echo "==> 4) Checking HDFS and uploading file directly"\n    # Upload file directly to HDFS since NiFi might not be configured\n    docker exec "${NAMENODE_CONTAINER}" hdfs dfs -mkdir -p "/marketing/sales" 2>/dev/null || true\n    docker exec "${NAMENODE_CONTAINER}" hdfs dfs -put -f "/data/sales.csv" "${HDFS_TARGET_PATH}" 2>/dev/null || echo "File might already exist in HDFS"\n\n    # Verify file exists\n    if docker exec "${NAMENODE_CONTAINER}" hdfs dfs -test -e "${HDFS_TARGET_PATH}" 2>/dev/null; then\n    echo "✅ File confirmed in HDFS at ${HDFS_TARGET_PATH}"\n    else\n    echo "⚠️  File not found in HDFS, but continuing..."\n    fi\n\n    echo "==> 5) Copying Spark app and JAR into spark-master and running it"\n    docker exec "${SPARK_MASTER_CONTAINER}" mkdir -p "${SPARK_APP_DIR}/libs" || true\n    docker cp "/opt/***/marketing_job.py" "${SPARK_MASTER_CONTAINER}:${SPARK_APP_PATH}" || echo "marketing_job.py not found"\n    docker cp "/opt/***/libs/postgresql-42.6.0.jar" "${SPARK_MASTER_CONTAINER}:${SPARK_APP_DIR}/libs/postgresql-42.6.0.jar" || echo "postgresql jar not found"\n\n    docker exec "${SPARK_MASTER_CONTAINER}"     "${SPARK_HOME}/bin/spark-submit"     --master "${SPARK_MASTER_URL}"     --jars "${SPARK_APP_DIR}/libs/postgresql-42.6.0.jar"     "${SPARK_APP_PATH}" || echo "Spark job failed"\n\n    echo "==> 6) Verifying Postgres load in table: ${PG_TABLE}"\n    docker exec -i "${PG_CONTAINER}" bash -c     "PGPASSWORD=\'${PG_PASS}\' psql -U ${PG_USER} -d ${PG_DB} -c \'SELECT * FROM ${PG_TABLE} LIMIT 5;\'" || echo "Postgres verification failed"\n\n    echo "✅ Pipeline finished."\nEOF\n\n        chmod +x /tmp/pipeline.sh\n        /tmp/pipeline.sh\n        ']
[2025-08-25T02:18:56.726+0000] {subprocess.py:86} INFO - Output:
[2025-08-25T02:18:56.728+0000] {subprocess.py:93} INFO - === Debugging file locations ===
[2025-08-25T02:18:56.728+0000] {subprocess.py:93} INFO - Contents of /opt/***/dags:
[2025-08-25T02:18:56.730+0000] {subprocess.py:93} INFO - total 24
[2025-08-25T02:18:56.731+0000] {subprocess.py:93} INFO - drwxr-xr-x+ 4    1000 1000 4096 Aug 25 00:46 .
[2025-08-25T02:18:56.731+0000] {subprocess.py:93} INFO - drwxrwxr-x  1 *** root 4096 Aug 25 02:07 ..
[2025-08-25T02:18:56.731+0000] {subprocess.py:93} INFO - drwxr-xr-x+ 2    1000 1000 4096 Aug 22 16:25 __pycache__
[2025-08-25T02:18:56.732+0000] {subprocess.py:93} INFO - -rw-rw-rw-  1    1000 1000 5166 Aug 25 01:53 daily_pipeline_dag.py
[2025-08-25T02:18:56.732+0000] {subprocess.py:93} INFO - drwxrwxrwx+ 3    1000 1000 4096 Aug 25 00:21 scripts
[2025-08-25T02:18:56.732+0000] {subprocess.py:93} INFO - Contents of /opt/***/dags/scripts:
[2025-08-25T02:18:56.733+0000] {subprocess.py:93} INFO - total 28
[2025-08-25T02:18:56.733+0000] {subprocess.py:93} INFO - drwxrwxrwx+ 3    1000 1000 4096 Aug 25 00:21 .
[2025-08-25T02:18:56.734+0000] {subprocess.py:93} INFO - drwxr-xr-x+ 4    1000 1000 4096 Aug 25 00:46 ..
[2025-08-25T02:18:56.734+0000] {subprocess.py:93} INFO - drwxrwxrwx+ 2 *** root 4096 Aug 24 13:41 __pycache__
[2025-08-25T02:18:56.734+0000] {subprocess.py:93} INFO - -rw-rw-rw-  1    1000 1000 8115 Aug 25 00:21 daily_pipeline.sh
[2025-08-25T02:18:56.735+0000] {subprocess.py:93} INFO - -rwxrwxrwx  1 *** root 4156 Aug 24 14:41 pipeline.sh
[2025-08-25T02:18:56.735+0000] {subprocess.py:93} INFO - Searching for pipeline.sh:
[2025-08-25T02:18:56.737+0000] {subprocess.py:93} INFO - /opt/***/dags/scripts/pipeline.sh
[2025-08-25T02:18:56.737+0000] {subprocess.py:93} INFO - === Testing file access ===
[2025-08-25T02:18:56.739+0000] {subprocess.py:93} INFO - ls: cannot access '/opt/***/data/sales.csv': No such file or directory
[2025-08-25T02:18:56.739+0000] {subprocess.py:93} INFO - File not found
[2025-08-25T02:18:56.739+0000] {subprocess.py:93} INFO - File check failed
[2025-08-25T02:18:56.745+0000] {subprocess.py:93} INFO - ==> 1) Checking dataset exists: /opt/***/data/sales.csv
[2025-08-25T02:18:56.745+0000] {subprocess.py:93} INFO - ERROR: /opt/***/data/sales.csv not found.
[2025-08-25T02:18:56.745+0000] {subprocess.py:97} INFO - Command exited with return code 1
[2025-08-25T02:18:56.755+0000] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/bash.py", line 210, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-08-25T02:18:56.759+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=daily_pipeline, task_id=run_pipeline, execution_date=20250825T021354, start_date=20250825T021856, end_date=20250825T021856
[2025-08-25T02:18:56.768+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 278 for task run_pipeline (Bash command failed. The command returned a non-zero exit code 1.; 66)
[2025-08-25T02:18:56.788+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-08-25T02:18:56.802+0000] {taskinstance.py:2778} INFO - 0 downstream tasks scheduled from follow-on schedule check
