[2025-08-25T01:31:50.278+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: daily_pipeline.run_pipeline __airflow_temporary_run_2025-08-25T01:21:28.510251+00:00__ [queued]>
[2025-08-25T01:31:50.288+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: daily_pipeline.run_pipeline __airflow_temporary_run_2025-08-25T01:21:28.510251+00:00__ [queued]>
[2025-08-25T01:31:50.288+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 2
[2025-08-25T01:31:50.298+0000] {taskinstance.py:1382} INFO - Executing <Task(BashOperator): run_pipeline> on 2025-08-25 00:00:00+00:00
[2025-08-25T01:31:50.302+0000] {standard_task_runner.py:57} INFO - Started process 57 to run task
[2025-08-25T01:31:50.305+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'daily_pipeline', 'run_pipeline', '__***_temporary_run_2025-08-25T01:21:28.510251+00:00__', '--job-id', '269', '--raw', '--subdir', 'DAGS_FOLDER/daily_pipeline_dag.py', '--cfg-path', '/tmp/tmpnsnc6nqu']
[2025-08-25T01:31:50.306+0000] {standard_task_runner.py:85} INFO - Job 269: Subtask run_pipeline
[2025-08-25T01:31:50.348+0000] {task_command.py:416} INFO - Running <TaskInstance: daily_pipeline.run_pipeline __airflow_temporary_run_2025-08-25T01:21:28.510251+00:00__ [running]> on host 88d490cfafa3
[2025-08-25T01:31:50.414+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='ecommerce' AIRFLOW_CTX_DAG_ID='daily_pipeline' AIRFLOW_CTX_TASK_ID='run_pipeline' AIRFLOW_CTX_EXECUTION_DATE='2025-08-25T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='__***_temporary_run_2025-08-25T01:21:28.510251+00:00__'
[2025-08-25T01:31:50.416+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-08-25T01:31:50.416+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', '\n    cd /opt/***\n\n    # Debug: Check what files are available\n    echo "=== Debugging file locations ==="\n    echo "Contents of /opt/***/dags:"\n    ls -la /opt/***/dags/\n    echo "Contents of /opt/***/dags/scripts:"\n    ls -la /opt/***/dags/scripts/ || echo "dags/scripts not found"\n    echo "Searching for pipeline.sh:"\n    find /opt/*** -name "pipeline.sh" -type f || echo "pipeline.sh not found anywhere"\n\n    # The docker-compose mounts ./scripts to /opt/***/dags/scripts, overriding the dags/scripts\n    # So we need to copy our script from the dags mount to a temp location\n\n    # Create a temporary script with the correct paths\n    cat > /tmp/pipeline.sh << \'EOF\'\n#!/usr/bin/env bash\nset -euo pipefail\n\n### --------- CONFIG (edit if needed) ---------\nDATASET_LOCAL_PATH="/opt/***/data/input/sales.csv"\nNIFI_LOCAL_INBOX="/opt/***/data"\nHDFS_TARGET_PATH="/marketing/sales/sales.csv"\n\n# NiFi\nNIFI_URL="https://localhost:8443"\nNIFI_USER="admin"\nNIFI_PASS="admin123456789"\nNIFI_PROCESS_GROUP_ID="PUT_YOUR_PG_ID_HERE"\n\n# Spark\nSPARK_MASTER_CONTAINER="spark-master"\nSPARK_HOME="/opt/bitnami/spark"\nSPARK_APP_DIR="${SPARK_HOME}/app"\nSPARK_APP_PATH="${SPARK_APP_DIR}/marketing_job.py"\nSPARK_MASTER_URL="spark://spark-master:7077"\n\n# HDFS (inside namenode container)\nNAMENODE_CONTAINER="namenode"\n\n# Postgres (warehouse-db) verification\nPG_CONTAINER="warehouse-db"\nPG_DB="warehouse"\nPG_USER="warehouse_user"\nPG_PASS="warehouse_pass"\nPG_TABLE="sales_data"\n\n# Grafana (optional reload if you use provisioning)\nGRAFANA_URL="http://localhost:3000"\nGRAFANA_USER="admin"\nGRAFANA_PASS="admin"\n\necho "==> 1) Checking dataset exists: ${DATASET_LOCAL_PATH}"\nif [[ ! -f "${DATASET_LOCAL_PATH}" ]]; then\n  echo "ERROR: ${DATASET_LOCAL_PATH} not found."\n  exit 1\nfi\n\necho "==> 2) Placing dataset where NiFi can read it: ${NIFI_LOCAL_INBOX}"\ncp -f "${DATASET_LOCAL_PATH}" "${NIFI_LOCAL_INBOX}/sales.csv"\n\necho "==> 3) Starting NiFi process group to ingest CSV into HDFS"\ncurl -k -u "${NIFI_USER}:${NIFI_PASS}" -X PUT   "${NIFI_URL}/nifi-api/flow/process-groups/${NIFI_PROCESS_GROUP_ID}"   -H "Content-Type: application/json"   -d "{"id":"${NIFI_PROCESS_GROUP_ID}","state":"RUNNING"}" >/dev/null || echo "NiFi step skipped"\n\necho "==> 4) Checking HDFS and uploading file directly"\n# Upload file directly to HDFS since NiFi might not be configured\ndocker exec "${NAMENODE_CONTAINER}" hdfs dfs -mkdir -p "/marketing/sales" 2>/dev/null || true\ndocker exec "${NAMENODE_CONTAINER}" hdfs dfs -put -f "/data/sales.csv" "${HDFS_TARGET_PATH}" 2>/dev/null || echo "File might already exist in HDFS"\n\n# Verify file exists\nif docker exec "${NAMENODE_CONTAINER}" hdfs dfs -test -e "${HDFS_TARGET_PATH}" 2>/dev/null; then\n  echo "✅ File confirmed in HDFS at ${HDFS_TARGET_PATH}"\nelse\n  echo "⚠️  File not found in HDFS, but continuing..."\nfi\n\necho "==> 5) Copying Spark app and JAR into spark-master and running it"\ndocker exec "${SPARK_MASTER_CONTAINER}" mkdir -p "${SPARK_APP_DIR}/libs" || true\ndocker cp "/opt/***/marketing_job.py" "${SPARK_MASTER_CONTAINER}:${SPARK_APP_PATH}" || echo "marketing_job.py not found"\ndocker cp "/opt/***/libs/postgresql-42.6.0.jar" "${SPARK_MASTER_CONTAINER}:${SPARK_APP_DIR}/libs/postgresql-42.6.0.jar" || echo "postgresql jar not found"\n\ndocker exec "${SPARK_MASTER_CONTAINER}"   "${SPARK_HOME}/bin/spark-submit"   --master "${SPARK_MASTER_URL}"   --jars "${SPARK_APP_DIR}/libs/postgresql-42.6.0.jar"   "${SPARK_APP_PATH}" || echo "Spark job failed"\n\necho "==> 6) Verifying Postgres load in table: ${PG_TABLE}"\ndocker exec -i "${PG_CONTAINER}" bash -c   "PGPASSWORD=\'${PG_PASS}\' psql -U ${PG_USER} -d ${PG_DB} -c \'SELECT * FROM ${PG_TABLE} LIMIT 5;\'" || echo "Postgres verification failed"\n\necho "✅ Pipeline finished."\nEOF\n\n    chmod +x /tmp/pipeline.sh\n    /tmp/pipeline.sh\n    ']
[2025-08-25T01:31:50.425+0000] {subprocess.py:86} INFO - Output:
[2025-08-25T01:31:50.427+0000] {subprocess.py:93} INFO - === Debugging file locations ===
[2025-08-25T01:31:50.428+0000] {subprocess.py:93} INFO - Contents of /opt/***/dags:
[2025-08-25T01:31:50.430+0000] {subprocess.py:93} INFO - total 24
[2025-08-25T01:31:50.430+0000] {subprocess.py:93} INFO - drwxr-xr-x+ 4    1000 1000 4096 Aug 25 00:46 .
[2025-08-25T01:31:50.430+0000] {subprocess.py:93} INFO - drwxrwxr-x  1 *** root 4096 Aug 25 01:29 ..
[2025-08-25T01:31:50.430+0000] {subprocess.py:93} INFO - drwxr-xr-x+ 2    1000 1000 4096 Aug 22 16:25 __pycache__
[2025-08-25T01:31:50.430+0000] {subprocess.py:93} INFO - -rw-rw-rw-  1    1000 1000 4622 Aug 25 01:13 daily_pipeline_dag.py
[2025-08-25T01:31:50.430+0000] {subprocess.py:93} INFO - drwxrwxrwx+ 3    1000 1000 4096 Aug 25 00:21 scripts
[2025-08-25T01:31:50.431+0000] {subprocess.py:93} INFO - Contents of /opt/***/dags/scripts:
[2025-08-25T01:31:50.433+0000] {subprocess.py:93} INFO - total 28
[2025-08-25T01:31:50.433+0000] {subprocess.py:93} INFO - drwxrwxrwx+ 3    1000 1000 4096 Aug 25 00:21 .
[2025-08-25T01:31:50.433+0000] {subprocess.py:93} INFO - drwxr-xr-x+ 4    1000 1000 4096 Aug 25 00:46 ..
[2025-08-25T01:31:50.433+0000] {subprocess.py:93} INFO - drwxrwxrwx+ 2 *** root 4096 Aug 24 13:41 __pycache__
[2025-08-25T01:31:50.433+0000] {subprocess.py:93} INFO - -rw-rw-rw-  1    1000 1000 8115 Aug 25 00:21 daily_pipeline.sh
[2025-08-25T01:31:50.433+0000] {subprocess.py:93} INFO - -rwxrwxrwx  1 *** root 4156 Aug 24 14:41 pipeline.sh
[2025-08-25T01:31:50.434+0000] {subprocess.py:93} INFO - Searching for pipeline.sh:
[2025-08-25T01:31:50.436+0000] {subprocess.py:93} INFO - /opt/***/dags/scripts/pipeline.sh
[2025-08-25T01:31:50.442+0000] {subprocess.py:93} INFO - ==> 1) Checking dataset exists: /opt/***/data/input/sales.csv
[2025-08-25T01:31:50.442+0000] {subprocess.py:93} INFO - ERROR: /opt/***/data/input/sales.csv not found.
[2025-08-25T01:31:50.443+0000] {subprocess.py:97} INFO - Command exited with return code 1
[2025-08-25T01:31:50.454+0000] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/bash.py", line 210, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-08-25T01:31:50.457+0000] {taskinstance.py:1400} INFO - Marking task as UP_FOR_RETRY. dag_id=daily_pipeline, task_id=run_pipeline, execution_date=20250825T000000, start_date=20250825T013150, end_date=20250825T013150
[2025-08-25T01:31:50.465+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 269 for task run_pipeline (Bash command failed. The command returned a non-zero exit code 1.; 57)
[2025-08-25T01:31:50.478+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-08-25T01:31:50.492+0000] {taskinstance.py:2778} INFO - 0 downstream tasks scheduled from follow-on schedule check
