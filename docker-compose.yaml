version: '3.8'

services:
  # ================= HADOOP HDFS =================
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: unless-stopped
    ports:
      - "9870:9870"
      - "9000:9000"
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=data-cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    env_file:
      - ./hadoop.env
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - data-pipeline

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: unless-stopped
    ports:
      - "9864:9864"
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      - SERVICE_PRECONDITION=namenode:9870
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    env_file:
      - ./hadoop.env
    depends_on:
      namenode:
        condition: service_healthy
    networks:
      - data-pipeline

  # ================= SPARK WITH GREAT EXPECTATIONS =================
  spark-master:
    image: bitnami/spark:3.4
    container_name: spark-master
    command: bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=0.0.0.0
    volumes:
      - ./spark-jobs:/opt/spark-jobs
      - ./gx:/opt/gx
      - ./data:/opt/data
    restart: unless-stopped
    networks:
      - data-pipeline

  spark-worker:
    image: bitnami/spark:3.4
    container_name: spark-worker
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
    volumes:
      - ./spark-jobs:/opt/spark-jobs
      - ./gx:/opt/gx
      - ./data:/opt/data
    restart: unless-stopped
    networks:
      - data-pipeline

  # ================= GREAT EXPECTATIONS SERVICE =================
  great-expectations:
    build:
      context: ./docker/great-expectations
      dockerfile: Dockerfile
    container_name: great-expectations
    restart: unless-stopped
    # Removing port 5000 here as the dedicated gx-ui service will serve the docs on 8081
    # ports: 
    #   - "5000:5000"  # Data Docs UI 
    volumes:
      - ./gx:/opt/gx
      - ./data:/opt/data
      - ./spark-jobs:/opt/spark-jobs
    environment:
      - POSTGRES_HOST=warehouse-db
      - POSTGRES_PORT=5432
      - POSTGRES_DB=warehouse
      - POSTGRES_USER=warehouse_user
      - POSTGRES_PASSWORD=warehouse_pass
    depends_on:
      warehouse-db:
        condition: service_healthy
    networks:
      - data-pipeline
    command: tail -f /dev/null  # Keep container running

  # ================= NIFI =================
  nifi:
    image: apache/nifi:1.26.0
    container_name: nifi
    restart: unless-stopped
    ports:
      - "8443:8443"
    environment:
      - NIFI_WEB_HTTPS_PORT=8443
      - NIFI_WEB_HTTPS_HOST=0.0.0.0
      - NIFI_CLUSTER_IS_NODE=false
      - NIFI_SENSITIVE_PROPS_KEY=12345678901234567890A
      - SINGLE_USER_CREDENTIALS_USERNAME=admin
      - SINGLE_USER_CREDENTIALS_PASSWORD=admin123456789
      - NIFI_SECURITY_USER_AUTHORIZER=single-user-authorizer
      - NIFI_SECURITY_USER_LOGIN_IDENTITY_PROVIDER=single-user-provider
    volumes:
      - ./data:/data
      - ./config/hadoop/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml:ro
      - ./config/hadoop/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml:ro
      - nifi_database_repository:/opt/nifi/nifi-current/database_repository
      - nifi_flowfile_repository:/opt/nifi/nifi-current/flowfile_repository
      - nifi_content_repository:/opt/nifi/nifi-current/content_repository
      - nifi_provenance_repository:/opt/nifi/nifi-current/provenance_repository
      - nifi_state:/opt/nifi/nifi-current/state
      - nifi_logs:/opt/nifi/nifi-current/logs
      - nifi_conf:/opt/nifi/nifi-current/conf
    networks:
      - data-pipeline

  # ================= POSTGRES WAREHOUSE =================
  warehouse-db:
    image: postgres:15
    container_name: warehouse-db
    environment:
      POSTGRES_USER: warehouse_user
      POSTGRES_PASSWORD: warehouse_pass
      POSTGRES_DB: warehouse
    ports:
      - "5433:5432"
    volumes:
      - warehouse_data:/var/lib/postgresql/data
      - ./scripts/init-warehouse.sql:/docker-entrypoint-initdb.d/init.sql
    restart: unless-stopped
    networks:
      - data-pipeline
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U warehouse_user -d warehouse"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ================= GRAFANA =================
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-piechart-panel,grafana-worldmap-panel
    volumes:
      - grafana_data:/var/lib/grafana
      - ./config/grafana/provisioning:/etc/grafana/provisioning
      - ./config/grafana/dashboards:/var/lib/grafana/dashboards
    depends_on:
      warehouse-db:
        condition: service_healthy
    networks:
      - data-pipeline
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

# ================= VOLUMES =================
volumes:
  hadoop_namenode:
  hadoop_datanode:
  nifi_database_repository:
  nifi_flowfile_repository:
  nifi_content_repository:
  nifi_provenance_repository:
  nifi_state:
  nifi_logs:
  nifi_conf:
  warehouse_data:
  grafana_data:

# ================= NETWORKS =================
networks:
  data-pipeline:
    driver: bridge

