version: '3.8'

services:
  # ================= HADOOP HDFS =================
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: unless-stopped
    ports:
      - "9870:9870"
      - "9000:9000"
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=data-cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    env_file:
      - ./hadoop.env
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - data-pipeline

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: unless-stopped
    ports:
      - "9864:9864"
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      - SERVICE_PRECONDITION=namenode:9870
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    env_file:
      - ./hadoop.env
    depends_on:
      namenode:
        condition: service_healthy
    networks:
      - data-pipeline

  # ================= SPARK WITH GREAT EXPECTATIONS =================
  spark-master:
    image: bitnami/spark:3.4
    container_name: spark-master
    command: bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=0.0.0.0
    volumes:
      - ./spark-jobs:/opt/spark-jobs
      - ./gx:/opt/gx
      - ./data:/opt/data
      - ./scripts:/opt/airflow/scripts
    restart: unless-stopped
    networks:
      - data-pipeline

  spark-worker:
    image: bitnami/spark:3.4
    container_name: spark-worker
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
    volumes:
      - ./spark-jobs:/opt/spark-jobs
      - ./gx:/opt/gx
      - ./data:/opt/data
      - ./scripts:/opt/airflow/scripts
    restart: unless-stopped
    networks:
      - data-pipeline

  # ================= GREAT EXPECTATIONS SERVICE =================
  great-expectations:
    build:
      context: ./docker/great-expectations
      dockerfile: Dockerfile
    container_name: great-expectations
    restart: unless-stopped
    # Removing port 5000 here as the dedicated gx-ui service will serve the docs on 8081
    # ports: 
    #   - "5000:5000"  # Data Docs UI 
    volumes:
      - ./gx:/opt/gx
      - ./data:/opt/data
      - ./spark-jobs:/opt/spark-jobs
    environment:
      - POSTGRES_HOST=warehouse-db
      - POSTGRES_PORT=5432
      - POSTGRES_DB=warehouse
      - POSTGRES_USER=warehouse_user
      - POSTGRES_PASSWORD=warehouse_pass
    depends_on:
      warehouse-db:
        condition: service_healthy
    networks:
      - data-pipeline
    command: tail -f /dev/null  # Keep container running

  # ================= NIFI =================
  nifi:
    image: apache/nifi:1.26.0
    container_name: nifi
    restart: unless-stopped
    ports:
      - "8443:8443"
    environment:
      - NIFI_WEB_HTTPS_PORT=8443
      - NIFI_WEB_HTTPS_HOST=0.0.0.0
      - NIFI_CLUSTER_IS_NODE=false
      - NIFI_SENSITIVE_PROPS_KEY=12345678901234567890A
      - SINGLE_USER_CREDENTIALS_USERNAME=admin
      - SINGLE_USER_CREDENTIALS_PASSWORD=admin123456789
      - NIFI_SECURITY_USER_AUTHORIZER=single-user-authorizer
      - NIFI_SECURITY_USER_LOGIN_IDENTITY_PROVIDER=single-user-provider
    volumes:
      - ./data:/data
      - ./config/hadoop/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml:ro
      - ./config/hadoop/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml:ro
      - nifi_database_repository:/opt/nifi/nifi-current/database_repository
      - nifi_flowfile_repository:/opt/nifi/nifi-current/flowfile_repository
      - nifi_content_repository:/opt/nifi/nifi-current/content_repository
      - nifi_provenance_repository:/opt/nifi/nifi-current/provenance_repository
      - nifi_state:/opt/nifi/nifi-current/state
      - nifi_logs:/opt/nifi/nifi-current/logs
      - nifi_conf:/opt/nifi/nifi-current/conf
    networks:
      - data-pipeline

  # ================= POSTGRES WAREHOUSE =================
  warehouse-db:
    image: postgres:15
    container_name: warehouse-db
    environment:
      POSTGRES_USER: warehouse_user
      POSTGRES_PASSWORD: warehouse_pass
      POSTGRES_DB: warehouse
    ports:
      - "5433:5432"
    volumes:
      - warehouse_data:/var/lib/postgresql/data
      - ./scripts/init-warehouse.sql:/docker-entrypoint-initdb.d/init.sql
    restart: unless-stopped
    networks:
      - data-pipeline
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U warehouse_user -d warehouse"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ================= GRAFANA =================
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-piechart-panel,grafana-worldmap-panel
    volumes:
      - grafana_data:/var/lib/grafana
      - ./config/grafana/provisioning:/etc/grafana/provisioning
      - ./config/grafana/dashboards:/var/lib/grafana/dashboards
    depends_on:
      warehouse-db:
        condition: service_healthy
    networks:
      - data-pipeline
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
  
    # ================= AIRFLOW =================
  airflow-redis:
    image: redis:latest
    container_name: airflow-redis
    restart: unless-stopped
    networks:
      - data-pipeline
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 128M

  airflow-postgres:
    image: postgres:15
    container_name: airflow-postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    restart: unless-stopped
    volumes:
      - airflow_db:/var/lib/postgresql/data
    networks:
      - data-pipeline

  airflow-init:
    image: apache/airflow:2.9.1
    container_name: airflow-init
    entrypoint: /bin/bash
    command: >
      -c "airflow db migrate &&
          airflow users create --username admin --password admin --firstname Air --lastname Flow --role Admin --email admin@example.com || true"
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://airflow-redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@airflow-postgres/airflow
      AIRFLOW__WEBSERVER__SECRET_KEY: t3K5q9Nh8R_4sV2bX7uP1zL0aY6mC8wQ
    user: "50000:50000"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./data:/opt/airflow/data
      - ./scripts:/opt/airflow/scripts
    depends_on:
      - airflow-postgres
      - airflow-redis
    networks:
      - data-pipeline
    restart: "no"
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M

  airflow-scheduler:
    image: apache/airflow:2.9.1
    container_name: airflow-scheduler
    command: scheduler
    restart: unless-stopped
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://airflow-redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@airflow-postgres/airflow
      AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: 300
      AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT: "False"
      AIRFLOW__WEBSERVER__SECRET_KEY: t3K5q9Nh8R_4sV2bX7uP1zL0aY6mC8wQ
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./data:/opt/airflow/data
      - ./scripts:/opt/airflow/scripts
    depends_on:
      - airflow-init
    networks:
      - data-pipeline
    deploy:
      resources:
        limits:
          cpus: '0.75'
          memory: 768M
        reservations:
          cpus: '0.5'
          memory: 512M

  airflow-webserver:
    image: apache/airflow:2.9.1
    container_name: airflow-webserver
    command: webserver
    restart: unless-stopped
    ports:
      - "8081:8080"   # Airflow UI
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://airflow-redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@airflow-postgres/airflow
      AIRFLOW__WEBSERVER__WORKERS: 2
      AIRFLOW__WEBSERVER__WORKER_CLASS: sync
      AIRFLOW__WEBSERVER__TIMEOUT: 300
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "True"
      AIRFLOW__WEBSERVER__SECRET_KEY: t3K5q9Nh8R_4sV2bX7uP1zL0aY6mC8wQ
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./data:/opt/airflow/data
      - ./scripts:/opt/airflow/scripts
    depends_on:
      - airflow-init
    networks:
      - data-pipeline
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 120s

  airflow-worker:
    image: apache/airflow:2.9.1
    container_name: airflow-worker
    command: celery worker
    restart: unless-stopped
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://airflow-redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@airflow-postgres/airflow
      AIRFLOW__CELERY__WORKER_CONCURRENCY: 2
      AIRFLOW__WEBSERVER__SECRET_KEY: t3K5q9Nh8R_4sV2bX7uP1zL0aY6mC8wQ
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./data:/opt/airflow/data
      - ./scripts:/opt/airflow/scripts
    depends_on:
      - airflow-init
    networks:
      - data-pipeline
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M


# ================= VOLUMES =================
volumes:
  hadoop_namenode:
  hadoop_datanode:
  nifi_database_repository:
  nifi_flowfile_repository:
  nifi_content_repository:
  nifi_provenance_repository:
  nifi_state:
  nifi_logs:
  nifi_conf:
  warehouse_data:
  grafana_data:
  airflow_db:
  airflow_dags:

# ================= NETWORKS =================
networks:
  data-pipeline:
    driver: bridge

